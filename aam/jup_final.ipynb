{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 assets, index the returned LazyList to import.\n",
      "[====================] 100% (2000/2000) - done.                                 \n",
      "- Computing reference shape                                                     Computing batch 0\n",
      "- Building modelsges size: [==========] 100% (2000/2000) - done.                \n",
      "  - Scale 0: Doneding appearance model                                          maining\n",
      "  - Scale 1: Doneding appearance model                                          \n",
      "                                                              "
     ]
    }
   ],
   "source": [
    "import menpo.io as mio\n",
    "from menpo.visualize import print_progress\n",
    "from menpo.landmark import labeller, face_ibug_68_to_face_ibug_68_trimesh\n",
    "from menpofit.aam import HolisticAAM\n",
    "from menpo.feature import fast_dsift\n",
    "from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n",
    "import numpy as np\n",
    "import menpo\n",
    "\n",
    "path_to_images = '/home/rshaydu/tensorfaces/helen/trainset/'\n",
    "training_images = []\n",
    "for img in print_progress(mio.import_images(path_to_images, verbose=True)):\n",
    "    # convert to greyscale\n",
    "    if img.n_channels == 3:\n",
    "        img = img.as_greyscale()\n",
    "    # crop to landmarks bounding box with an extra 20% padding\n",
    "    img = img.crop_to_landmarks_proportion(0.2)\n",
    "    # rescale image if its diagonal is bigger than 400 pixels\n",
    "    d = img.diagonal()\n",
    "    if d > 400:\n",
    "        img = img.rescale(400.0 / d)\n",
    "    # define a TriMesh which will be useful for Piecewise Affine Warp of HolisticAAM\n",
    "    labeller(img, 'PTS', face_ibug_68_to_face_ibug_68_trimesh)\n",
    "    # append to list\n",
    "    training_images.append(img)\n",
    "\n",
    "aam = HolisticAAM(training_images, group='face_ibug_68_trimesh', diagonal=150,\n",
    "                  scales=(0.5, 1.0), holistic_features=fast_dsift, verbose=True,\n",
    "                  max_shape_components=20, max_appearance_components=150)\n",
    "\n",
    "\n",
    "fitter = LucasKanadeAAMFitter(aam, lk_algorithm_cls=WibergInverseCompositional,\n",
    "                              n_shape=[5, 20], n_appearance=[30, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarking  /home/rshaydu/tensorfaces/FaceBase_png/yakov-vp1-il2-ex2.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'min_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d9cb24866dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# # Detect face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0minitial_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'min_size'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from menpodetect import load_dlib_frontal_face_detector\n",
    "\n",
    "\n",
    "# Load and convert to grayscale\n",
    "image_folder = '/home/rshaydu/tensorfaces/FaceBase_png/'\n",
    "img_files = list(filter(lambda x: \".png\" in x, os.listdir(image_folder)))\n",
    "\n",
    "# somewhere here a for loop starts\n",
    "\n",
    "img_path = image_folder+img_files[15]\n",
    "\n",
    "print(\"Landmarking \", img_path)\n",
    "\n",
    "image = mio.import_image(img_path)\n",
    "image = image.as_greyscale()\n",
    "\n",
    "# Load detector\n",
    "detect = load_dlib_frontal_face_detector()\n",
    "\n",
    "# # Detect face\n",
    "bboxes = detect(image,min_size=(150, 150))\n",
    "initial_box = None\n",
    "if len(bboxes) > 0:\n",
    "    initial_box = bboxes[0]\n",
    "    print(\"dlib_frontal_face_detector found an initial box\")\n",
    "else:\n",
    "# initial bbox\n",
    "# build box by hand\n",
    "    adjacency_matrix = np.array([[0,1,0,0],\n",
    "                                 [0,0,1,0],\n",
    "                                 [0,0,0,1],\n",
    "                                 [1,0,0,0]])\n",
    "\n",
    "    points = None\n",
    "    if 'vp0' in img_path or 'vp1' in img_path:\n",
    "        points = np.array([[150,100],[500,100],[500,300],[150,300]]) # fine for looking right\n",
    "    elif 'vp3' in img_path or 'vp4' in img_path:\n",
    "        points = np.array([[120,50],[500,50],[500,250],[120,250]])\n",
    "    elif 'vp0' in img_path:\n",
    "        points = np.array([[120,75],[500,75],[500,275],[120,275]])\n",
    "\n",
    "    if points == None:\n",
    "        print(\"Error: incorrect finename\", img_path)\n",
    "        sys.exit(0)\n",
    "\n",
    "    initial_bbox = menpo.shape.PointDirectedGraph(points, adjacency_matrix)\n",
    "    print(\"Using a hardcoded initial box\")\n",
    "\n",
    "# fit image\n",
    "result = fitter.fit_from_bb(image, initial_bbox, max_iters=50)\n",
    "\n",
    "# print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.view(render_initial_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped = fitter.warped_images(result.image, [result.final_shape, fitter.reference_shape])\n",
    "warped[0].view()\n",
    "# mio.export_image(cropped,\"/home/rshaydu/tmp/out.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menpo",
   "language": "python",
   "name": "menpo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
