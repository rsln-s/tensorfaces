{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 assets, index the returned LazyList to import.\n",
      "[====                ] 20% (403/2000) - 00:04:04 remaining                      "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected dtype (object) - normalisation range is unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2680f982b781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpath_to_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/rshaydu/tensorfaces/helen/trainset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtraining_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# convert to greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/visualize/textutils.py\u001b[0m in \u001b[0;36mprint_progress\u001b[0;34m(iterable, prefix, n_items, offset, show_bar, show_count, show_eta, end_with_newline, min_seconds_between_updates)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mlast_update_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, slice_)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__index__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# PEP 357 and single integer index access - returns element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;31m# A slice or unknown type is passed - let List handle it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/io/input/base.py\u001b[0m in \u001b[0;36m_import\u001b[0;34m(filepath, extensions_map, landmark_resolver, landmark_ext_map, landmark_attach_func, asset, importer_kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimporter_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mimporter_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m     \u001b[0mbuilt_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimporter_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimporter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;31m# landmarks are iterable so check for list precisely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/io/input/image.py\u001b[0m in \u001b[0;36mpillow_importer\u001b[0;34m(filepath, asset, normalize, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Greyscale, Integer and RGB images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         image = Image.init_from_channels_at_back(\n\u001b[0;32m---> 76\u001b[0;31m             _pil_to_numpy(pil_image, normalize))\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Convert to 'L' type (http://stackoverflow.com/a/4114122/1716869).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/io/input/image.py\u001b[0m in \u001b[0;36m_pil_to_numpy\u001b[0;34m(pil_image, normalize, convert)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnormalize_pixels_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/menpo/lib/python3.5/site-packages/menpo/image/base.py\u001b[0m in \u001b[0;36mnormalize_pixels_range\u001b[0;34m(pixels, error_on_unknown_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_on_unknown_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise ValueError('Unexpected dtype ({}) - normalisation range '\n\u001b[0;32m--> 104\u001b[0;31m                              'is unknown'.format(dtype))\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Do nothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected dtype (object) - normalisation range is unknown"
     ]
    }
   ],
   "source": [
    "import menpo.io as mio\n",
    "from menpo.visualize import print_progress\n",
    "from menpo.landmark import labeller, face_ibug_68_to_face_ibug_68_trimesh\n",
    "from menpofit.aam import HolisticAAM\n",
    "from menpo.feature import fast_dsift\n",
    "from menpofit.aam import LucasKanadeAAMFitter, WibergInverseCompositional\n",
    "import numpy as np\n",
    "import menpo\n",
    "\n",
    "path_to_images = '/home/rshaydu/tensorfaces/helen/trainset/'\n",
    "training_images = []\n",
    "for img in print_progress(mio.import_images(path_to_images, verbose=True)):\n",
    "    # convert to greyscale\n",
    "    if img.n_channels == 3:\n",
    "        img = img.as_greyscale()\n",
    "    # crop to landmarks bounding box with an extra 20% padding\n",
    "    img = img.crop_to_landmarks_proportion(0.2)\n",
    "    # rescale image if its diagonal is bigger than 400 pixels\n",
    "    d = img.diagonal()\n",
    "    if d > 400:\n",
    "        img = img.rescale(400.0 / d)\n",
    "    # define a TriMesh which will be useful for Piecewise Affine Warp of HolisticAAM\n",
    "    labeller(img, 'PTS', face_ibug_68_to_face_ibug_68_trimesh)\n",
    "    # append to list\n",
    "    training_images.append(img)\n",
    "\n",
    "aam = HolisticAAM(training_images, group='face_ibug_68_trimesh', diagonal=150,\n",
    "                  scales=(0.5, 1.0), holistic_features=fast_dsift, verbose=True,\n",
    "                  max_shape_components=20, max_appearance_components=150)\n",
    "\n",
    "\n",
    "fitter = LucasKanadeAAMFitter(aam, lk_algorithm_cls=WibergInverseCompositional,\n",
    "                              n_shape=[5, 20], n_appearance=[30, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from menpodetect import load_dlib_frontal_face_detector\n",
    "\n",
    "\n",
    "# Load and convert to grayscale\n",
    "image_folder = '/home/rshaydu/tensorfaces/FaceBase_png/'\n",
    "img_files = list(filter(lambda x: \".png\" in x, os.listdir(image_folder)))\n",
    "\n",
    "# somewhere here a for loop starts\n",
    "\n",
    "for img_file in img_files:\n",
    "    img_path = image_folder+img_file\n",
    "\n",
    "    print(\"Landmarking \", img_path)\n",
    "\n",
    "    image = mio.import_image(img_path)\n",
    "    image = image.as_greyscale()\n",
    "\n",
    "    # Load detector\n",
    "    detect = load_dlib_frontal_face_detector()\n",
    "\n",
    "    # # Detect face\n",
    "    bboxes = detect(image)\n",
    "    initial_box = None\n",
    "    if len(bboxes) > 0:\n",
    "        initial_bbox = bboxes[0]\n",
    "        print(\"dlib_frontal_face_detector found an initial box\")\n",
    "    else:\n",
    "    # initial bbox\n",
    "    # build box by hand\n",
    "        adjacency_matrix = np.array([[0,1,0,0],\n",
    "                                     [0,0,1,0],\n",
    "                                     [0,0,0,1],\n",
    "                                     [1,0,0,0]])\n",
    "\n",
    "        points = None\n",
    "        if 'vp0' in img_path or 'vp1' in img_path:\n",
    "            points = np.array([[150,100],[500,100],[500,300],[150,300]]) # fine for looking right\n",
    "        elif 'vp3' in img_path or 'vp4' in img_path:\n",
    "            points = np.array([[120,50],[500,50],[500,250],[120,250]])\n",
    "        elif 'vp0' in img_path:\n",
    "            points = np.array([[120,75],[500,75],[500,275],[120,275]])\n",
    "\n",
    "        if points == None:\n",
    "            print(\"Error: incorrect finename\", img_path)\n",
    "            sys.exit(0)\n",
    "\n",
    "        initial_bbox = menpo.shape.PointDirectedGraph(points, adjacency_matrix)\n",
    "        print(\"Using a hardcoded initial box\")\n",
    "\n",
    "    # fit image\n",
    "    result = fitter.fit_from_bb(image, initial_bbox, max_iters=200)\n",
    "\n",
    "    # print result\n",
    "    print(result)\n",
    "\n",
    "    warped = fitter.warped_images(result.image, [result.final_shape, fitter.reference_shape])\n",
    "    # warped[0].view()\n",
    "    mio.export_image(warped[0],\"/home/rshaydu/tensorfaces/FaceBase_warped/\"+img_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menpo",
   "language": "python",
   "name": "menpo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
